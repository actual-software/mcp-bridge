#!/usr/bin/env python3
"""
Batch Processing Manager
Manages parallel lint-fixer agents processing issues in batches
"""

import json
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import threading

class BatchProcessor:
    def __init__(self, max_parallel=10):
        self.max_parallel = max_parallel
        self.issues_file = Path('audit-results/extracted_issues.json')
        self.results_file = Path('audit-results/batch_results.json')
        self.progress_lock = threading.Lock()
        self.stats = {
            'total_batches': 0,
            'completed_batches': 0,
            'successful_fixes': 0,
            'failed_fixes': 0,
            'start_time': time.time()
        }
    
    def load_issues(self):
        """Load extracted issues and batches"""
        if not self.issues_file.exists():
            raise FileNotFoundError(f"Issues file not found: {self.issues_file}")
        
        with open(self.issues_file, 'r') as f:
            data = json.load(f)
        
        return data
    
    def process_single_issue(self, issue):
        """Process a single issue (this would normally spawn a Task agent)"""
        # For now, let's simulate the agent processing
        # In the actual implementation, this would use the Task tool
        
        issue_prompt = f"""
ISSUE_ID: {issue['id']}
FILE_PATH: {issue['file_path']}
LINE_NUMBER: {issue['line_number']}
COLUMN_NUMBER: {issue['column_number']}
LINTER_NAME: {issue['linter_name']}
ISSUE_DESCRIPTION: {issue['description']}
MODULE_PATH: {issue['module_path']}
BATCH_ID: {issue.get('batch_id', 'unknown')}

Fix this specific linting issue without affecting other code.
Report back with standardized success/failure format.
"""
        
        # Simulate processing time (real agent would take longer)
        time.sleep(0.1)  
        
        # For demo, return success for most issues
        if "Critical security" in issue['description'] or issue['linter_name'] == 'gosec':
            # Security issues might be more complex
            success_rate = 0.8
        else:
            success_rate = 0.9
            
        import random
        if random.random() < success_rate:
            return {
                'issue_id': issue['id'],
                'status': 'SUCCESS',
                'file': f"{issue['file_path']}:{issue['line_number']}",
                'linter': issue['linter_name'],
                'change': f"Fixed {issue['linter_name']} issue"
            }
        else:
            return {
                'issue_id': issue['id'],
                'status': 'FAILED', 
                'file': f"{issue['file_path']}:{issue['line_number']}",
                'linter': issue['linter_name'],
                'error': "Complex refactoring required"
            }
    
    def process_batch(self, batch):
        """Process a single batch of issues"""
        batch_id = batch['batch_id']
        batch_start = time.time()
        
        print(f"\nüîß Processing {batch_id}: {len(batch['issues'])} issues")
        
        results = []
        successful = 0
        failed = 0
        
        # Process issues in the batch
        for issue in batch['issues']:
            result = self.process_single_issue(issue)
            results.append(result)
            
            if result['status'] == 'SUCCESS':
                successful += 1
                print(f"  ‚úÖ {result['issue_id']}: {result['linter']} fix applied")
            else:
                failed += 1
                print(f"  ‚ùå {result['issue_id']}: {result['linter']} - {result['error']}")
        
        batch_duration = time.time() - batch_start
        
        # Update global stats
        with self.progress_lock:
            self.stats['completed_batches'] += 1
            self.stats['successful_fixes'] += successful
            self.stats['failed_fixes'] += failed
        
        batch_result = {
            'batch_id': batch_id,
            'status': 'COMPLETED',
            'duration': batch_duration,
            'issues_processed': len(batch['issues']),
            'successful_fixes': successful,
            'failed_fixes': failed,
            'results': results
        }
        
        print(f"  üìä {batch_id} complete: {successful}/{len(batch['issues'])} fixed ({batch_duration:.1f}s)")
        
        return batch_result
    
    def print_progress_update(self, batch_num=None):
        """Print current progress"""
        with self.progress_lock:
            elapsed = time.time() - self.stats['start_time']
            completed = self.stats['completed_batches']
            total = self.stats['total_batches']
            success = self.stats['successful_fixes']
            failed = self.stats['failed_fixes']
            total_issues = success + failed
            
            if total > 0:
                batch_progress = f"{completed}/{total}"
                success_rate = f"{(success/(total_issues) if total_issues > 0 else 0)*100:.1f}%"
            else:
                batch_progress = "0/0"
                success_rate = "0%"
        
        print(f"\nüîç LINT SCAN PROGRESS - {batch_progress} batches")
        print(f"Issues resolved: {success} | Failed: {failed} | Success rate: {success_rate}")
        print(f"Elapsed time: {elapsed/60:.1f}m | Active agents: processing")
    
    def run_batch_processing(self, max_batches=10):
        """Run parallel batch processing"""
        print("üöÄ STARTING SYSTEMATIC LINTING CLEANUP")
        print("=" * 60)
        
        # Load issues
        data = self.load_issues()
        batches = data['batches']
        
        # Limit for demo
        batches_to_process = batches[:max_batches]
        self.stats['total_batches'] = len(batches_to_process)
        
        print(f"üìã Processing first {len(batches_to_process)} batches ({len(batches_to_process) * 10} issues)")
        print(f"üîß Max parallel agents: {self.max_parallel}")
        print(f"üìà Priority order: Security ‚Üí Errors ‚Üí Imports ‚Üí Format ‚Üí Style ‚Üí Performance")
        
        all_results = []
        
        # Process batches in parallel
        with ThreadPoolExecutor(max_workers=self.max_parallel) as executor:
            # Submit all batch jobs
            future_to_batch = {
                executor.submit(self.process_batch, batch): batch 
                for batch in batches_to_process
            }
            
            # Process completed batches
            for future in as_completed(future_to_batch):
                try:
                    result = future.result()
                    all_results.append(result)
                    
                    # Print periodic progress
                    if len(all_results) % 5 == 0:
                        self.print_progress_update()
                        
                except Exception as e:
                    batch = future_to_batch[future]
                    print(f"‚ùå Batch {batch['batch_id']} failed with error: {e}")
        
        # Final summary
        self.print_final_summary(all_results)
        
        # Save results
        final_data = {
            'processing_summary': self.stats,
            'batch_results': all_results,
            'timestamp': time.time()
        }
        
        with open(self.results_file, 'w') as f:
            json.dump(final_data, f, indent=2)
        
        print(f"\nüíæ Results saved to: {self.results_file}")
        
        return all_results
    
    def print_final_summary(self, results):
        """Print comprehensive final summary"""
        elapsed = time.time() - self.stats['start_time']
        total_issues = sum(r['issues_processed'] for r in results)
        total_success = sum(r['successful_fixes'] for r in results) 
        total_failed = sum(r['failed_fixes'] for r in results)
        avg_batch_time = sum(r['duration'] for r in results) / len(results) if results else 0
        success_rate = (total_success / total_issues * 100) if total_issues > 0 else 0
        
        print(f"\nüéØ BATCH PROCESSING COMPLETE - SUMMARY REPORT")
        print("=" * 60)
        print(f"‚è±Ô∏è  Total Duration: {elapsed/60:.1f} minutes")
        print(f"üìä Batches Processed: {len(results)}")
        print(f"üîß Issues Processed: {total_issues}")
        print(f"‚úÖ Successfully Fixed: {total_success}")
        print(f"‚ùå Failed to Fix: {total_failed}")
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"‚ö° Avg Batch Time: {avg_batch_time:.1f}s")
        print(f"üöÄ Throughput: {total_issues/elapsed*60:.1f} issues/minute")

def main():
    processor = BatchProcessor(max_parallel=10)
    
    # Process first 10 batches as demo
    processor.run_batch_processing(max_batches=10)

if __name__ == '__main__':
    main()